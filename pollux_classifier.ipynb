{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalawinka/politics_classifier/blob/main/pollux_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkrL-3VS4Hyk"
      },
      "source": [
        "# Detecting scientific articles from the political science field using scientific abstracts\n",
        "\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this tutorial, you will be able to\n",
        "\n",
        "- detect scientific article from the political science domain using pretrained models and the transformers library\n",
        "\n",
        "\n",
        "## Description\n",
        "This tutorial provides detailed information detecting texts from the political science domain using scientific abstracts. The usage of two models will be described in this tutorial.\n",
        "\n",
        "### 1. English classification model to detect texts from the political science domain\n",
        "\n",
        "The model was fine-tuned using a dataset of 2,919 abstracts from scientific articles retrieved from the [BASE](https://www.base-search.net/) and [POLLUX](https://www.pollux-fid.de/) collections of scientific articles. Based on [SSCI-SciBERT](https://link.springer.com/article/10.1007/s11192-022-04602-4).\n",
        "\n",
        "The BASE data were labelled as \"politics\" or \"multi\" according to the Dewey Decimal Classification (DDC). Data from several major political science journals in the POLLUX dataset were marked as \"politics\" class.\n",
        "\n",
        "Accuracy: 0.9\n",
        "\n",
        "Predicts 2 classes:\n",
        "\n",
        "| class |description| precision | recall | f1-score | support |\n",
        "|:------|:------------|:------------|:---------|:-----------|:----------|\n",
        "| multi    | other scientific domains|   0.911 | 0.899 |   0.905 |   513 |\n",
        "| politics | political science|   0.889 | 0.902      |   0.895 |       460 |\n",
        "\n",
        "To learn more about the model see model card: https://huggingface.co/kalawinka/SSciBERT_politics\n",
        "\n",
        "### 2. Multilingual classification model to detect texts from the political science domain\n",
        "\n",
        "\n",
        "This model is a multilingual version of our [SSciBERT_politics](https://huggingface.co/kalawinka/SSciBERT_politics). Based on [BERT multilingual base model (uncased)](http://arxiv.org/abs/1810.04805).\n",
        "\n",
        "The model was fine-tuned using a dataset of 14,178 abstracts from scientific articles retrieved from the [BASE](https://www.base-search.net/)\n",
        "and [POLLUX](https://www.pollux-fid.de/) collections of scientific articles.\n",
        "Abstracts from scientific articles in 3 languages (English, German and French) were used for the training.\n",
        "The BASE data were labelled as \"politics\" or \"multi\" according to the Dewey Decimal Classification (DDC).\n",
        "Data from several major political science journals in the POLLUX dataset were marked as \"politics\" class.\n",
        "\n",
        "Accuracy: 0.978\n",
        "\n",
        "Predicts 2 classes in 3 languages (English, German and French):\n",
        "\n",
        "| class    | description              | precision | recall | f1-score | support |\n",
        "|----------|--------------------------|-----------|--------|----------|---------|\n",
        "| politics |political science         | 0.975     | 0.978  | 0.976    | 2143    |\n",
        "| multi    |other scientific domains  | 0.981     | 0.979  | 0.980    | 2583    |\n",
        "\n",
        "Evaluation by class and language:\n",
        "\n",
        "| class    | description              | language | precision | recall | f1-score | support |\n",
        "|----------|--------------------------|----------|-----------|--------|----------|---------|\n",
        "| politics | political science        | English  | 0,989     | 0,993  | 0,991    | 1212    |\n",
        "| multi    | other scientific domains | English  | 0,992     | 0,989  | 0,991    | 1164    |\n",
        "| politics | political science        | German   | 0,952     | 0,958  | 0,955    | 783     |\n",
        "| multi    | other scientific domains | German   | 0,957     | 0,951  | 0,954    | 776     |\n",
        "| politics | political science        | French   | 0,979     | 0,959  | 0,969    | 148     |\n",
        "| multi    | other scientific domains | French   | 0,991     | 0,995  | 0,993    | 643     |\n",
        "\n",
        "To learn more about the model see model card: https://huggingface.co/kalawinka/bert-base-ml-politics\n",
        "\n",
        "## Target Audience (Difficulty level)\n",
        "- This tutorial is aimed at beginners with some knowledge in Python.\n",
        "\n",
        "## Prerequisites\n",
        "- Prior knowledge of Python programming is required for this tutorial\n",
        "- Prior knowledge of Jupyter/IPython Notebook usage is required for this tutorial\n",
        "    - To learn more see: https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/\n",
        "- Prior knowledge of Google Colab usage is required for this tutorial\n",
        "    - To learn more see: https://colab.research.google.com/drive/16pBJQePbqkz3QFV54L4NIkOn1kwpuRrj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VMXZxe14Hym"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        " - In order to run this tutorial, you need at least Python >= 3.11.4  \n",
        " - The following dependencies should also install the required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TY8nmlm4Hym"
      },
      "outputs": [],
      "source": [
        "# Install packages for Jupiter Notebook environment\n",
        "!pip3 install transformers\n",
        "!pip3 install datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNafT38X4Hyn"
      },
      "source": [
        "# How to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41OHYB2U4Hyn"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSXuSSaK4Hyn"
      },
      "source": [
        "## 1. English classification model to detect texts from the political science domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYlxcj-p4Hyn"
      },
      "outputs": [],
      "source": [
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('kalawinka/SSciBERT_politics')\n",
        "# load classification model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('kalawinka/SSciBERT_politics')\n",
        "# initiaize classification pipeline\n",
        "# optional argument device: https://huggingface.co/docs/transformers/en/main_classes/pipelines#:~:text=device%20(int%20or%20str%20or%20torch.device)%20%E2%80%94%20Defines%20the%20device%20(e.g.%2C%20%22cpu%22%2C%20%22cuda%3A1%22%2C%20%22mps%22%2C%20or%20a%20GPU%20ordinal%20rank%20like%201)%20on%20which%20this%20pipeline%20will%20be%20allocated.\n",
        "pipe = pipeline(\"text-classification\", model=model, tokenizer = tokenizer, max_length=512, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWVgzCxk4Hyn"
      },
      "source": [
        "### OR TRY\n",
        "## 2. Multilingual classification model to detect texts from the political science domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plqH7VqQ4Hyo"
      },
      "outputs": [],
      "source": [
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('kalawinka/bert-base-ml-politics')\n",
        "# load classification model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('kalawinka/bert-base-ml-politics')\n",
        "# initiaize classification pipeline\n",
        "# optional argument device: https://huggingface.co/docs/transformers/en/main_classes/pipelines#:~:text=device%20(int%20or%20str%20or%20torch.device)%20%E2%80%94%20Defines%20the%20device%20(e.g.%2C%20%22cpu%22%2C%20%22cuda%3A1%22%2C%20%22mps%22%2C%20or%20a%20GPU%20ordinal%20rank%20like%201)%20on%20which%20this%20pipeline%20will%20be%20allocated.\n",
        "pipe = pipeline(\"text-classification\", model=model, tokenizer = tokenizer, max_length=512, truncation=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0LY_SId4Hyo"
      },
      "source": [
        "## Try model (English or multilingual) with a single text\n",
        "### Sample Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe9gNd8T4Hyo",
        "outputId": "9d352d8a-ebdb-4445-a85a-986c71375c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'politics', 'score': 0.9995762705802917}]\n"
          ]
        }
      ],
      "source": [
        "# use classification pipline with a single text\n",
        "example = 'Germaparl is a collection of protocols of 72 years of parliamentary debates in the German Bundestag. Analysis of parliamentary debates can reveal the hidden programmatic-ideological positions of political parties. From the linguistic perspective, parliamentary debates can be analysed in terms of political discourse, sentiment and position-taking, and cross-cultural and gender differences. Our main focus is the analysis of interjections or ’Zwischenrufe’ and calls to order or ’Ordnungsrufe’. Calls to order are a good tool to analyse the negativity in politics. Moreover, we believe that both calls to order and interjections are able to reveal tendencies in the country’s political scene, collaboration patterns between political parties and the impact and productivity of a single politician or political party. To our best knowledge calls to order and interjections in the Germaparl corpus were not analysed by other researchers.'\n",
        "out = pipe(example)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f00j4Q0k4Hyo"
      },
      "source": [
        "### Output data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qisw9O164Hyp",
        "outputId": "a688375b-b81f-4572-f1dc-e66dd0c95246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "politics\n",
            "0.9995762705802917\n"
          ]
        }
      ],
      "source": [
        "# access model output single features\n",
        "for el in out:\n",
        "    # access classification label\n",
        "    print(el.get('label'))\n",
        "    # access score\n",
        "    print(el.get('score'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6NQfWEr4Hyp"
      },
      "source": [
        "## Apply model to corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8VRjLkv4Hyp"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TME3Jmdc4Hyp"
      },
      "source": [
        "### Sample Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOMC4mAv4Hyp"
      },
      "outputs": [],
      "source": [
        "#create example corpus\n",
        "data_dict = {\n",
        "    'id': ['1', '2', '3', '4'],\n",
        "    'text' : [\n",
        "        'Germaparl is a collection of protocols of 72 years of parliamentary debates in the German Bundestag. Analysis of parliamentary debates can reveal the hidden programmatic-ideological positions of political parties. From the linguistic perspective, parliamentary debates can be analysed in terms of political discourse, sentiment and position-taking, and cross-cultural and gender differences. Our main focus is the analysis of interjections or ’Zwischenrufe’ and calls to order or ’Ordnungsrufe’. Calls to order are a good tool to analyse the negativity in politics. Moreover, we believe that both calls to order and interjections are able to reveal tendencies in the country’s political scene, collaboration patterns between political parties and the impact and productivity of a single politician or political party. To our best knowledge calls to order and interjections in the Germaparl corpus were not analysed by other researchers.',\n",
        "        'Acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends. The aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers. We trained and implemented a named entity recognition (NER) task using the Flair NLP framework. The training was conducted using three default Flair NER models with four differently-sized corpora and different versions of the Flair NLP framework. The Flair Embeddings model trained on the medium corpus with the latest FLAIR version showed the best accuracy of 0.79. Expanding the size of a training corpus from very small to medium size massively increased the accuracy of all training algorithms, but further expansion of the training corpus did not bring further improvement. Moreover, the performance of the model slightly deteriorated. Our model is able to recognize six entity types: funding agency, grant number, individuals, university, corporation, and miscellaneous. The model works more precisely for some entity types than for others; thus, individuals and grant numbers showed a very good F1-Score over 0.9. Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data. This model can be applied for the comprehensive analysis of acknowledgment texts and may potentially make a great contribution to the field of automated acknowledgment analysis.',\n",
        "        'Analysis of acknowledgments is particularly interesting as acknowledgments may give information not only about funding, but they are also able to reveal hidden contributions to authorship and the researcher’s collaboration patterns, context in which research was conducted, and specific aspects of the academic work. The focus of the present research is the analysis of a large sample of acknowledgement texts indexed in the Web of Science (WoS) Core Collection. Record types “article” and “review” from four different scientific domains, namely social sciences, economics, oceanography and computer science, published from 2014 to 2019 in a scientific journal in English were considered. Six types of acknowledged entities, i.e., funding agency, grant number, individuals, university, corporation and miscellaneous, were extracted from the acknowledgement texts using a Named Entity Recognition (NER) tagger and subsequently examined. A general analysis of the acknowledgement texts showed that indexing of funding information in WoS is incomplete. The analysis of the automatically extracted entities revealed differences and distinct patterns in the distribution of acknowledged entities of different types between different scientific domains. A strong association was found between acknowledged entity and scientific domain, and acknowledged entity and entity type. Only negligible correlation was found between the number of citations and the number of acknowledged entities. Generally, the number of words in the acknowledgement texts positively correlates with the number of acknowledged funding organizations, universities, individuals and miscellaneous entities. At the same time, acknowledgement texts with the larger number of sentences have more acknowledged individuals and miscellaneous categories.',\n",
        "        'Purpose: The recent proliferation of preprints could be a way for researchers worldwide to increase the availability and visibility of their research findings. Against the background of rising publication costs caused by the increasing prevalence of article processing fees, the search for other ways to publish research results besides traditional journal publication may increase. This could be especially true for lower-income countries. Design/methodology/approach: Therefore, we are interested in the experiences and attitudes towards posting and using preprints in the Global South as opposed to the Global North. To explore whether motivations and concerns about posting preprints differ, we adopted a mixed-methods approach, combining a quantitative survey of researchers with focus group interviews. Findings: We found that respondents from the Global South were more likely to agree to adhere to policies and to emphasise that mandates could change publishing behaviour towards open access. They were also more likely to agree posting preprints has a positive impact. Respondents from the Global South and the Global North emphasised the importance of peer-reviewed research for career advancement. Originality: The study has identified a wide range of experiences with and attitudes towards posting preprints among researchers in the Global South and the Global North. To our knowledge, this has hardly been studied before, which is also because preprints only have emerged lately in many disciplines and countries.'\n",
        "    ]\n",
        "}\n",
        "# convert dictionary to transformers dataset\n",
        "# learn more about the dataset class: https://huggingface.co/docs/datasets/en/create_dataset\n",
        "dataset = Dataset.from_dict(data_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF6P7QVv4Hyp",
        "outputId": "156943e3-c7f0-4238-aa88-16e6b282135e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'text'],\n",
            "    num_rows: 4\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzBx_ofJ4Hyp",
        "outputId": "594d47e4-6aba-4036-e21c-27e0216c541d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 4/4 [00:01<00:00,  3.28 examples/s]\n"
          ]
        }
      ],
      "source": [
        "#apply classification pipeline to the whole corpus and save results as a dictionary in the dataset column 'multi_clas'\n",
        "encoded_dataset = dataset.map(\n",
        "        lambda x: {\"multi_clas\": pipe(\n",
        "                x['text'])},batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntebT7Hs4Hyp"
      },
      "source": [
        "### Output data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGS_tjH-4Hyp",
        "outputId": "c019fa85-209a-46e6-a3a2-e39bfbd53bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "Germaparl is a collection of protocols of 72 years of parliamentary debates in the German Bundestag. Analysis of parliamentary debates can reveal the hidden programmatic-ideological positions of political parties. From the linguistic perspective, parliamentary debates can be analysed in terms of political discourse, sentiment and position-taking, and cross-cultural and gender differences. Our main focus is the analysis of interjections or ’Zwischenrufe’ and calls to order or ’Ordnungsrufe’. Calls to order are a good tool to analyse the negativity in politics. Moreover, we believe that both calls to order and interjections are able to reveal tendencies in the country’s political scene, collaboration patterns between political parties and the impact and productivity of a single politician or political party. To our best knowledge calls to order and interjections in the Germaparl corpus were not analysed by other researchers.\n",
            "politics\n",
            "0.9995762705802917\n",
            "2\n",
            "Acknowledgments in scientific papers may give an insight into aspects of the scientific community, such as reward systems, collaboration patterns, and hidden research trends. The aim of the paper is to evaluate the performance of different embedding models for the task of automatic extraction and classification of acknowledged entities from the acknowledgment text in scientific papers. We trained and implemented a named entity recognition (NER) task using the Flair NLP framework. The training was conducted using three default Flair NER models with four differently-sized corpora and different versions of the Flair NLP framework. The Flair Embeddings model trained on the medium corpus with the latest FLAIR version showed the best accuracy of 0.79. Expanding the size of a training corpus from very small to medium size massively increased the accuracy of all training algorithms, but further expansion of the training corpus did not bring further improvement. Moreover, the performance of the model slightly deteriorated. Our model is able to recognize six entity types: funding agency, grant number, individuals, university, corporation, and miscellaneous. The model works more precisely for some entity types than for others; thus, individuals and grant numbers showed a very good F1-Score over 0.9. Most of the previous works on acknowledgment analysis were limited by the manual evaluation of data and therefore by the amount of processed data. This model can be applied for the comprehensive analysis of acknowledgment texts and may potentially make a great contribution to the field of automated acknowledgment analysis.\n",
            "multi\n",
            "0.9998723268508911\n",
            "3\n",
            "Analysis of acknowledgments is particularly interesting as acknowledgments may give information not only about funding, but they are also able to reveal hidden contributions to authorship and the researcher’s collaboration patterns, context in which research was conducted, and specific aspects of the academic work. The focus of the present research is the analysis of a large sample of acknowledgement texts indexed in the Web of Science (WoS) Core Collection. Record types “article” and “review” from four different scientific domains, namely social sciences, economics, oceanography and computer science, published from 2014 to 2019 in a scientific journal in English were considered. Six types of acknowledged entities, i.e., funding agency, grant number, individuals, university, corporation and miscellaneous, were extracted from the acknowledgement texts using a Named Entity Recognition (NER) tagger and subsequently examined. A general analysis of the acknowledgement texts showed that indexing of funding information in WoS is incomplete. The analysis of the automatically extracted entities revealed differences and distinct patterns in the distribution of acknowledged entities of different types between different scientific domains. A strong association was found between acknowledged entity and scientific domain, and acknowledged entity and entity type. Only negligible correlation was found between the number of citations and the number of acknowledged entities. Generally, the number of words in the acknowledgement texts positively correlates with the number of acknowledged funding organizations, universities, individuals and miscellaneous entities. At the same time, acknowledgement texts with the larger number of sentences have more acknowledged individuals and miscellaneous categories.\n",
            "politics\n",
            "0.9509045481681824\n",
            "4\n",
            "Purpose: The recent proliferation of preprints could be a way for researchers worldwide to increase the availability and visibility of their research findings. Against the background of rising publication costs caused by the increasing prevalence of article processing fees, the search for other ways to publish research results besides traditional journal publication may increase. This could be especially true for lower-income countries. Design/methodology/approach: Therefore, we are interested in the experiences and attitudes towards posting and using preprints in the Global South as opposed to the Global North. To explore whether motivations and concerns about posting preprints differ, we adopted a mixed-methods approach, combining a quantitative survey of researchers with focus group interviews. Findings: We found that respondents from the Global South were more likely to agree to adhere to policies and to emphasise that mandates could change publishing behaviour towards open access. They were also more likely to agree posting preprints has a positive impact. Respondents from the Global South and the Global North emphasised the importance of peer-reviewed research for career advancement. Originality: The study has identified a wide range of experiences with and attitudes towards posting preprints among researchers in the Global South and the Global North. To our knowledge, this has hardly been studied before, which is also because preprints only have emerged lately in many disciplines and countries.\n",
            "multi\n",
            "0.9874494075775146\n"
          ]
        }
      ],
      "source": [
        "#access single classification result from the dataset\n",
        "for el in encoded_dataset:\n",
        "    #access text id\n",
        "    print(el['id'])\n",
        "    #access text\n",
        "    print(el['text'])\n",
        "    #access classification label\n",
        "    print(el['multi_clas'].get('label'))\n",
        "    #access score\n",
        "    print(el['multi_clas'].get('score'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRin2QIk4Hyp"
      },
      "source": [
        "## References\n",
        "- Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In J. Burstein, C. Doran, & T. Solorio (Eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 4171–4186). Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423\n",
        "- Shen, S., Liu, J., Lin, L. et al. SsciBERT: a pre-trained language model for social science texts. Scientometrics 128, 1241–1263 (2023). https://doi.org/10.1007/s11192-022-04602-4\n",
        "- https://huggingface.co/docs/transformers/index\n",
        "- https://huggingface.co/kalawinka/SSciBERT_politics\n",
        "- https://huggingface.co/kalawinka/bert-base-ml-politics\n",
        "- https://www.base-search.net/\n",
        "- https://www.pollux-fid.de/\n",
        "\n",
        "## Contact details\n",
        "Nina Smirnova \\\n",
        "Email: nina.smirnova@gesis.org \\\n",
        "Huggingface: https://huggingface.co/kalawinka \\\n",
        "Research intersets: NLP, Machine Learning, Computational Linguistics, LLMs, Text Minings"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}